{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![BTS](https://github.com/vfp1/bts-dsf-2020/raw/main/Logo-BTS.jpg)\n",
    "\n",
    "# 3rd_Assigment-C\n",
    "\n",
    "### Lenin Escobar <lenin.escobar@bts.tech> - Data Driven Business (2020-12-21)\n",
    "\n",
    "Open this notebook in Github: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lveagithub/bts-ddb-2020/blob/master/3rdDatascienceFinance/code/3rd_Assigment-C_Helper.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General purposes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import gc\n",
    "import warnings\n",
    "# sklearn preprocessing for dealing with categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "#Missing values\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "#Training\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#Metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import jaccard_score\n",
    "import itertools\n",
    "#Options\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:powderblue;\">General helper Classes</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleaningHelper():\n",
    "    \"\"\"Cleaning Helper\"\"\"\n",
    "    def __init__(self, version):\n",
    "        pd.options.mode.chained_assignment = None  # default='warn'\n",
    "        self.version = version\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Cleaning helper version {self.version}\"\n",
    "    \n",
    "    @contextmanager\n",
    "    def timer(title):\n",
    "        t0 = time.time()\n",
    "        yield\n",
    "        print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "    \n",
    "    def get_nulls_data(self, df_):\n",
    "        #We want to know the quality of data. \n",
    "        #So, let's start by detecting not null percentage related to every column. \n",
    "\n",
    "        df_tot_nulls = df_.isnull().sum().sort_values(ascending=False)\n",
    "        df_tot_nulls_perc = 100 - round(df_tot_nulls/len(df_)*100,2)\n",
    "        df_tot_perc_nulls = pd.concat([df_tot_nulls,df_tot_nulls_perc],axis=1)\n",
    "        df_tot_perc_nulls = df_tot_perc_nulls.rename(columns={0: \"Total\", 1: \"PercNotNull\"})\n",
    "        return df_tot_perc_nulls\n",
    "    \n",
    "    def load_training_data(self):\n",
    "        # Train Dataset\n",
    "        df_app_train = pd.read_csv('../home-credit-default-risk/dataset/application_train.csv')\n",
    "        print('Testing data shape: ', df_app_train.shape)\n",
    "        return df_app_train\n",
    "    \n",
    "    def load_testing_data(self):\n",
    "        # Test Dataset\n",
    "        df_app_test = pd.read_csv('../home-credit-default-risk/dataset/application_test.csv')\n",
    "        print('Testing data shape: ', df_app_test.shape)\n",
    "        return df_app_test\n",
    "    \n",
    "    def label_encoding(self,app_train,app_test):\n",
    "        \"\"\"Process encoded dataframes\n",
    "        Code taken from: \n",
    "        https://github.com/LZhemin/home-credit-default-risk/blob/master/3_PreProcessed_Models/PreProcessed_Models.ipynb\n",
    "        \"\"\"\n",
    "        # Create a label encoder object\n",
    "        le = LabelEncoder()\n",
    "        le_count = 0\n",
    "\n",
    "        # Iterate through the columns\n",
    "        for col in app_train:\n",
    "            if app_train[col].dtype == 'object':\n",
    "                # If 2 or fewer unique categories\n",
    "                if len(list(app_train[col].unique())) <= 2:\n",
    "                    # Train on the training data\n",
    "                    le.fit(app_train[col])\n",
    "                    # Transform both training and testing data\n",
    "                    app_train[col] = le.transform(app_train[col])\n",
    "                    app_test[col] = le.transform(app_test[col])\n",
    "\n",
    "                    # Keep track of how many columns were label encoded\n",
    "                    le_count += 1\n",
    "\n",
    "        print('%d columns were label encoded in test and train dataframes.' % le_count)\n",
    "\n",
    "    def one_hot_encoding(self,app_train,app_test):\n",
    "        \"\"\"Process one-hot-encoded dataframes\n",
    "        Code taken from: \n",
    "        https://github.com/LZhemin/home-credit-default-risk/blob/master/3_PreProcessed_Models/PreProcessed_Models.ipynb\n",
    "        \"\"\"\n",
    "        # one-hot encoding of categorical variables\n",
    "        app_train_dummies = pd.get_dummies(app_train)\n",
    "        app_test_dummies = pd.get_dummies(app_test)\n",
    "\n",
    "        print('Training Features shape: ', app_train_dummies.shape)\n",
    "        print('Testing Features shape: ', app_test_dummies.shape)\n",
    "        return app_train_dummies, app_test_dummies\n",
    "    \n",
    "    def align_train_test(self,app_train,app_test):\n",
    "        \"\"\"Process one-hot-encoded dataframes\n",
    "        Code taken from: \n",
    "        https://github.com/LZhemin/home-credit-default-risk/blob/master/3_PreProcessed_Models/PreProcessed_Models.ipynb\n",
    "        \"\"\"\n",
    "        train_labels = app_train['TARGET']\n",
    "\n",
    "        # Align the training and testing data, keep only columns present in both dataframes\n",
    "        app_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n",
    "\n",
    "        # Add the target back in\n",
    "        app_train['TARGET'] = train_labels\n",
    "\n",
    "        print('Training Features shape: ', app_train.shape)\n",
    "        print('Testing Features shape: ', app_test.shape)\n",
    "        return app_train, app_test\n",
    "\n",
    "    def missing_value_treatment(self,app_train,app_test):\n",
    "        # Drop the target from the training data\n",
    "        #print(app_train.shape)\n",
    "        #print(app_test.shape)\n",
    "        trainY = app_train['TARGET']\n",
    "\n",
    "        if 'TARGET' in app_train:\n",
    "            trainX = app_train.drop(columns = ['TARGET'])\n",
    "        else:\n",
    "            trainX = app_train.copy()\n",
    "\n",
    "        # Feature names\n",
    "        features = list(trainX.columns)\n",
    "\n",
    "        # Copy of the testing data\n",
    "        #test = app_test.copy()\n",
    "\n",
    "        # Median imputation of missing values\n",
    "        imputer = SimpleImputer(strategy = 'median')\n",
    "\n",
    "        # Scale each feature to 0-1\n",
    "        scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "        # Fit on the training data\n",
    "        imputer.fit(trainX)\n",
    "\n",
    "        # Transform both training and testing data\n",
    "        trainX = imputer.transform(trainX)\n",
    "        test = imputer.transform(app_test)\n",
    "\n",
    "        new_app_train = pd.DataFrame(data=trainX[:,:], columns=features[:])\n",
    "        new_app_train['TARGET'] = trainY\n",
    "\n",
    "        new_app_test = pd.DataFrame(data=test[:,:], columns=features[:])\n",
    "\n",
    "\n",
    "        print(trainX.shape)\n",
    "        print(trainY.shape)\n",
    "        print(new_app_train.shape)\n",
    "        print(new_app_test.shape)\n",
    "        return new_app_train, new_app_test\n",
    "    \n",
    "    def get_corralation(self, df_corr):\n",
    "        df_correlations = df_corr.corr()['TARGET'].sort_values()\n",
    "        print('Most Positive Correlations:\\n', df_correlations.tail(10))\n",
    "        print('\\n\\nMost Negative Correlations:\\n', df_correlations.head(10))\n",
    "        return df_correlations\n",
    "    \n",
    "    def make_polynomial_features(self,app_train,app_test):\n",
    "        poly_features = app_train\n",
    "        poly_features_test = app_test\n",
    "\n",
    "        poly_target = poly_features['TARGET']\n",
    "        poly_features = poly_features.drop(columns = ['TARGET'])\n",
    "        \n",
    "        print('Polynomial Features shape - Before: ', poly_features.shape)\n",
    "        print('Polynomial Target shape - Before: ', poly_target.shape)\n",
    "        print('Polynomial Test shape - Before: ', poly_features_test.shape)\n",
    "        \n",
    "        # Create the polynomial object with specified degree\n",
    "        poly_transformer = PolynomialFeatures(degree = 3)\n",
    "\n",
    "        # Train the polynomial features\n",
    "        poly_transformer.fit(poly_features)\n",
    "\n",
    "        # Transform the features\n",
    "        poly_features = poly_transformer.transform(poly_features)\n",
    "        poly_features_test = poly_transformer.transform(poly_features_test)\n",
    "\n",
    "        print('Polynomial Features shape - After: ', poly_features.shape)\n",
    "        print('Polynomial Target shape: - After', poly_target.shape)\n",
    "        print('Polynomial Test shape: - After', poly_features_test.shape)\n",
    "        \n",
    "        poly_features = pd.DataFrame(poly_features, \n",
    "                                     columns = poly_transformer.get_feature_names(['EXT_SOURCE_3', 'EXT_SOURCE_2','EXT_SOURCE_1', 'DAYS_BIRTH', 'REGION_RATING_CLIENT_W_CITY']))\n",
    "\n",
    "        # Add in the target\n",
    "        poly_features['TARGET'] = poly_target\n",
    "\n",
    "        # Find the correlations with the target\n",
    "        poly_corrs = poly_features.corr()['TARGET'].sort_values()\n",
    "\n",
    "        print('Polynomial Features shape: ', poly_features.shape)\n",
    "        print('Polynomial Target shape: ', poly_target.shape)\n",
    "        print('Polynomial Test shape: ', poly_features_test.shape)\n",
    "\n",
    "        # Display most negative and most positive\n",
    "        print(\"Most Negative Correlations:\")\n",
    "        print(poly_corrs.head(10))\n",
    "        print(\"\\nMost Positive Correlations:\")\n",
    "        print(poly_corrs.tail(10))        \n",
    "        return poly_features, poly_features_test\n",
    "    \n",
    "    def scaling_value_treatment(self, poly_features_train, poly_features_test):\n",
    "        seed = 10\n",
    "        trainY_poly = poly_features_train['TARGET']\n",
    "\n",
    "        if 'TARGET' in poly_features_train:\n",
    "            trainX_poly = poly_features_train.drop(columns = ['TARGET'])\n",
    "        else:\n",
    "            trainX_poly = poly_features_train.copy()\n",
    "\n",
    "        # Feature names\n",
    "        features = list(trainX_poly.columns)\n",
    "\n",
    "        # Copy of the testing data\n",
    "        test_poly = poly_features_test.copy()\n",
    "\n",
    "        # Median imputation of missing values\n",
    "        imputer = SimpleImputer(strategy = 'median')\n",
    "\n",
    "        # Scale each feature to 0-1\n",
    "        scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "        # Fit on the training data\n",
    "        imputer.fit(trainX_poly)\n",
    "\n",
    "        # Transform both training and testing data\n",
    "        trainX_poly = imputer.transform(trainX_poly)\n",
    "        test_poly = imputer.transform(test_poly)\n",
    "\n",
    "\n",
    "        # Repeat with the scaler\n",
    "        scaler.fit(trainX_poly)\n",
    "        trainX_poly = scaler.transform(trainX_poly)\n",
    "        test = scaler.transform(test_poly)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(trainX_poly, trainY_poly, test_size=0.33, random_state=seed)\n",
    "\n",
    "        print(trainX_poly.shape)\n",
    "        print(trainY_poly.shape)\n",
    "\n",
    "        print(X_train.shape)\n",
    "        print(y_train.shape)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def handling_imbalanced_data(self, X_train, y_train):\n",
    "        sm = SMOTE(random_state=2)\n",
    "        X_resampled, y_resampled = sm.fit_resample(X_train, y_train.ravel())\n",
    "\n",
    "        X_resampled\n",
    "        y_resampled\n",
    "\n",
    "        print(X_resampled.shape)\n",
    "        print(y_resampled.shape)\n",
    "        return X_resampled, y_resampled\n",
    "    \n",
    "    def imp_logistic_regression_model(self, X_train, y_train):\n",
    "        log_reg = LogisticRegression(C = 0.0001)\n",
    "        log_reg.fit(X_train, y_train)\n",
    "        return log_reg\n",
    "\n",
    "    def accuracy_logistic_regression_model(self, X_train, y_train, log_reg):\n",
    "        # get prediction accuracy\n",
    "        crossVal = cross_val_score(log_reg, X_train, y_train, cv=5)\n",
    "        print(\"Training Cross Validation: %0.2f\" % (sum(crossVal) / float(len(crossVal)) * 100), \"%\")\n",
    "        accuracy = log_reg.score(X_test, y_test)\n",
    "        print(\"Test Accuraccy: %0.2f\" % (accuracy * 100), \"%\")\n",
    "\n",
    "    def score_logistic_regression_model(self, y_test, y_predicted):    \n",
    "        #F1 Score\n",
    "        print(\"F1 Score: %0.2f\" % f1_score(y_test, y_predicted))\n",
    "        #the size of the intersection divided by the size of the union of two label sets. \n",
    "        #If the entire set of predicted labels for a sample strictly match with the true set of labels, \n",
    "        #then the subset accuracy is 1.0; otherwise it is 0.0.\n",
    "        jaccard_score_ = jaccard_score(y_test, y_predicted)\n",
    "        print(\"Jaccard score: %0.2f\" % jaccard_score_)\n",
    "\n",
    "    \n",
    "    def roc_logistic_regression_model(self, X_test, y_test):\n",
    "        scores = log_reg.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, _ = metrics.roc_curve(y_test, scores)\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "        return scores, fpr, tpr, roc_auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsfinalproject]",
   "language": "python",
   "name": "conda-env-dsfinalproject-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
